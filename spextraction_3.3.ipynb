{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ba6af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# made a copy so I can mess w/ programs to test the I/F cal\n",
    "\n",
    "# Define paths here! the function is run at the bottom of the file, including variable definitions here so you don't have to scroll all the way down.\n",
    "\n",
    "'''\n",
    "format_file = '/prvt/ilio/EZDisturbance/haze/cfk/calibcm_for_kennedi/spextraction_input_binned_lat_EZ.txt' # this will stay the same. file contents at end of notebook\n",
    "data_list = '/prvt/ilio/EZDisturbance/haze/cfk/calibcm_for_kennedi/data_list_test.txt' # This will be a new data list for each date\n",
    "map_save_location = '/prvt/ilio/EZDisturbance/haze/cfk/calibcm_for_kennedi/test_maps/' # change this directory for each date so they don't overwrite themselves\n",
    "specfile = './jupiter.spx' # change this name for each date\n",
    "solarfile = '/prvt/ilio/EZDisturbance/haze/cfk/calibcm_for_kennedi/kurucz_HST_IRTF.dat' # Emma will change this when running the wrapper/spextraction\n",
    "\n",
    "# defaults:\n",
    "meancm_path='/prvt/ilio/EZDisturbance/haze/'\n",
    "wavelength_keyword='OSF'\n",
    "plot_maps = 0\n",
    "map_load = 0 # will want to re-save maps for each date\n",
    "IF_scale=1 # change this depending on whether the images have already been calibrated or not\n",
    "file_format = 'Nemesis'\n",
    "IF_plot_save_location = './IF_plot.pdf # won't matter, we shouldn't save I/F plots for each image\n",
    "error_percent = 0.3 # assume 30% error\n",
    "save_IF_spec = 1 # will be nice to have I/F spectra to reference\n",
    "IF_file_save_location='/Users/emmadahl/Desktop/spex_2017_EZ_main_cloud_constraint_outputs/2019jun1_IF' # might want to update the name of each I/F file based on the date, maybe keep the path the same\n",
    "\n",
    "'''\n",
    "\n",
    "# variable definitions (not all are here, ctl f to find the others)\n",
    "'''\n",
    "format_file - string, path+name of input file for spectrum bin parameters. contains code for what kind of spectra to make. For now, only 1 format but will change that.\n",
    "data_list - text file containing paths+names of images from which to extract spectra. Proabbly just want one image per wavelength and grouped together in time (up to the user how to do that). Will sort by wavelength here so no need to do that in the input file. Will assume that the list contains images of the same target. \n",
    "plot_maps - on/off switch, int. Will plot maps if 1, not if 0.\n",
    "specfile - string, path+name of spectrum file to be made and saved. \n",
    "solarfile - file containing solar spectrum. \n",
    "\n",
    "# defaults:\n",
    "meancm_path - string, path of location of meancm*.sav files. Default here is where Glenn has them saved. \n",
    "map_load - int, on/off switch. 0, will save maps to path defined by map_save_location. 1, will load maps from that location.\n",
    "map_save_location - path to location of saved maps. \n",
    "IF_scale = int, 1 or 0. 1, will use python version of calibcm to scale the data to I/F, will automatically make an I/F plot and save it in IF_plot_save_location.\n",
    "file_format - string, label for desired format of spectrum file. currently only coded for Nemesis spx files.\n",
    "IF_plot_save_location - string, path+name of location to save I/F plots\n",
    "\n",
    "General warning - setting plot_maps=1 will produce LOTS of plots, which can be used to diagnose various problems. Be careful turning this on if your machine/network can't handle producing lots of plots.\n",
    "\n",
    "Feb. 2024 - v3.0 got rid of default 2 bins.\n",
    "v3.3 - adapted from spextraction_3.1_2017spex_testing_IF_cal. skipped over 3.2 on accident. Calibcm() and I/F->radiance calculation improved\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759515b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from scipy.interpolate import interp1d\n",
    "import math\n",
    "from pylanetary.navigation import *\n",
    "from pylanetary.utils import *\n",
    "from uncertainties.umath import *\n",
    "from uncertainties import unumpy\n",
    "from uncertainties import ufloat\n",
    "from scipy import interpolate\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from astroquery.jplhorizons import Horizons\n",
    "import astropy.units as u\n",
    "import math\n",
    "from scipy import ndimage\n",
    "from scipy import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087cc7ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The below functions will generate spectrum files to be used as input for a radiative transfer model. At the moment, the only avialable format is for NEMESIS, and much of the code is catered to IRTF SpeX Guidedog images. Should still be generally applicable and easy to add more options later. Individual functions have doc strings explaining inputs and outputs. To make spectra, run spectrum_file_maker() at the end of the notebook after compiling the other functions.\n",
    "\n",
    "To run this code:\n",
    "1. Install pylanetary into a new conda environment: https://github.com/emolter/pylanetary/tree/main\n",
    "2. Install uncertainties package into that environment: \n",
    "    $ conda activate pylanetary-tester\n",
    "    $ pip install uncertainties\n",
    "3. Run this code within that conda environment (to implement it within a jupyter notebook, use https://medium.com/@nrk25693/how-to-add-your-conda-environment-to-your-jupyter-notebook-in-just-4-steps-abeab8b8d084)\n",
    "\n",
    "Written by Emma Dahl and Kennedi White, 2023/2024\n",
    "\n",
    "There are comments flagged with !! throughout that should go into a big to-do list, or that might become issues later.\n",
    "Biggest oustanding issues:\n",
    "-in spectral extraction, issue w/ mu going above 1. I don't think those spectra are equally spaced in mu\n",
    "-Conversion to radiance/I/F, solar spectrum to use?\n",
    "-Adding more options for implementing specta, probably a readme file for using those input files\n",
    "-Eventually adding the ability to calculate azimuth and solar zenith angle to pylanetary; will make projposolar and its auxiliary code unnecessary\n",
    "!! Implement spectral error that's made earlier on into spec_file_maker().\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535c0868",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# functions used within map_maker() to generate maps.\n",
    "\n",
    "def interceptellip(a,b,alpha,beta,gamma,x0,y0,z0):\n",
    "    '''\n",
    "    ; **********************************************************\n",
    "    ; procedure to find the intercepts (if any) between the line\n",
    "    ;\n",
    "    ;  (x-x0)       (y-y0)      (z-z0)\n",
    "    ;  ------  =    ------   =  ------\n",
    "    ;  alpha         beta       gamma\n",
    "    ; \n",
    "    ; and the ellipsoid\n",
    "    ;\n",
    "    ;\n",
    "    ;  x^2    y^2    z^2   \n",
    "    ;  --- +  --- +  ---  = 1\n",
    "    ;  a^2    a^2    b^2\n",
    "    ;\n",
    "    ; Input variables\n",
    "    ;       a       real    ellipsoid semi-major axis\n",
    "    ;       b       real    ellipsoid semi-minor axis\n",
    "    ;       alpha   real    line x-gradient\n",
    "    ;       beta    real    line y-gradient\n",
    "    ;       gamma   real    line z-gradient\n",
    "    ;       x0      real    line x-intercept\n",
    "    ;       y0      real    line y-intercept\n",
    "    ;       z0      real    line z-intercept\n",
    "    ;\n",
    "    ; Output variables\n",
    "    ;       iflag   integer Set to 1 if line intercepts, set to -1  otherwise\n",
    "    ;       x(2)    real    x-intercepts\n",
    "    ;       y(2)    real    y-intercepts\n",
    "    ;       z(2)    real    z-intercepts\n",
    "    ;\n",
    "    ; Pat Irwin     11/2/07\n",
    "    ; Python conversion - Emma Dahl 3-8-19\n",
    "    ; **********************************************************\n",
    "    '''\n",
    "    a1 = 1.0/a**2 + (beta/(a*alpha))**2 + (gamma/(b*alpha))**2\n",
    "\n",
    "    b1 = (-2*x0*beta**2/alpha**2 + 2*beta*y0/alpha)/a**2\n",
    "    b1 = b1 + (-2*x0*gamma**2/alpha**2 + 2*gamma*z0/alpha)/b**2\n",
    "    \n",
    "    c1 = ((beta*x0/alpha)**2 - 2*beta*y0*x0/alpha + y0**2)/a**2\n",
    "    c1 = c1 + ((gamma*x0/alpha)**2 - 2*gamma*x0*z0/alpha + z0**2)/b**2 -1\n",
    "        \n",
    "    #;print,a1,1.0/a**2 + (gamma/(b*alpha))**2\n",
    "    #;print,b1,2*gamma*z0/(alpha*b**2)\n",
    "    #;print,c1,(y0/a)**2 + (z0/b)**2 -1\n",
    "    \n",
    "    xtest = b1**2 - 4*a1*c1\n",
    "\n",
    "    x = np.zeros(2)\n",
    "    y = np.zeros(2)\n",
    "    z = np.zeros(2)\n",
    "        \n",
    "    if xtest > 0.0:\n",
    "        iflag = 1\n",
    "        x[0] = (-b1 + np.sqrt(xtest))/(2*a1)\n",
    "        x[1] = (-b1 - np.sqrt(xtest))/(2*a1)\n",
    "        y[0] = y0 + (beta/alpha)*(x[0]-x0)\n",
    "        y[1] = y0 + (beta/alpha)*(x[1]-x0)\n",
    "        z[0] = z0 + (gamma/alpha)*(x[0]-x0)\n",
    "        z[1] = z0 + (gamma/alpha)*(x[1]-x0)\n",
    "\n",
    "        # testing to see if solution is on ellipsoid\n",
    "        test=np.ndarray(len(x))\n",
    "        for i in range(0,len(x)):\n",
    "            test[i] = (x[i]/a)**2 + (y[i]/a)**2 + (z[i]/b)**2\n",
    "        xtest1 = abs(test[0]-1.0)\n",
    "        xtest2 = abs(test[1]-1.0)\n",
    "        \n",
    "        err = 1e-5\n",
    "        \n",
    "        if xtest1 > err or xtest2 > err:\n",
    "            print('Problem in interceptellip - solution not on ellipsoid')\n",
    "            print('Test =',test)\n",
    "    else:\n",
    "        iflag = -1\n",
    "\n",
    "\n",
    "    return iflag,x,y,z\n",
    "\n",
    "interceptellip_vec = np.vectorize(interceptellip)\n",
    "\n",
    "def projposolar(Re,obl,epsilon,latsol,lonsol,se_lon,eoff,poff):\n",
    "    '''\n",
    "    ; ************************************************************\n",
    "    ; Procedure to find latitude and longitude and zenith angle of\n",
    "    ; intercept between line and ellipsoid\n",
    "    ;\n",
    "    ; Input variables\n",
    "    ;       Re      real    Equatorial Radius (arcsec)\n",
    "    ;       obl     real    Planetary oblateness\n",
    "    ;       epsilon real    Sub-observer (planetocentric) latitude\n",
    "    ;       latsol  real    Sub-solar planetocentric latitude\n",
    "    ;       lonsol  real    longitude difference between sub-solar and sub-observer\n",
    "    ;                       points.\n",
    "    ;       se_lon  real    Sub-observer longitude # added by Emma\n",
    "    ;       eoff    real    equatorial offset of beam (arcsec)\n",
    "    ;       poff    real    polar offset of beam (arcsec)\n",
    "    ;\n",
    "    ; Output variables\n",
    "    ;       iflag   integer Set to 1 if real intercept, -1 otherwise\n",
    "    ;       xlat    real    Planetocentric latitude\n",
    "    ;       longitude real  xlon+se_lon, offset of longitude added to sub-observer longitude # added by Emma\n",
    "    ;       xlon    real    Longitude\n",
    "    ;       zen     real    Zenith angle\n",
    "    ;       szen    real    Solar zenith angle\n",
    "    ;       aphi    real    local azimuth angle between sun and observer\n",
    "    ;\n",
    "    ; Pat Irwin     11/2/07\n",
    "    ; Python conversion - Emma Dahl 3/8/19\n",
    "    ; ************************************************************\n",
    "    '''\n",
    "    \n",
    "    dtr = np.pi/180.0 # radians/degrees\n",
    "    Rp = Re*(1.0-obl)\n",
    "    \n",
    "    x0 = 0.0\n",
    "    y0 = eoff\n",
    "    z0 = poff/np.cos(epsilon*dtr)\n",
    "    \n",
    "    alpha = np.sin(np.pi/2.0 - epsilon*dtr)\n",
    "    beta = 0.0\n",
    "    gamma = np.cos(np.pi/2.0 - epsilon*dtr)\n",
    "    \n",
    "    # commenting out to give program my own iflag map\n",
    "    iflag,x,y,z = interceptellip_vec(Re,Rp,alpha,beta,gamma,x0,y0,z0)\n",
    "    #print(iflag)\n",
    "    \n",
    "    xlat = 0.0\n",
    "    xlon = 0.0\n",
    "    zen = 0.0\n",
    "    \n",
    "    # !!! getting rid of iflag statement here because I already have the maps set up for real intercepts below\n",
    "    #if iflag > 0:\n",
    "    # if real intercept, find lat, long, and zenith\n",
    "\n",
    "    # find distance along line of sight\n",
    "    lambdaa = (x-x0)/alpha\n",
    "    if lambdaa[0] > lambdaa[1]:\n",
    "        inear = 0\n",
    "    else:\n",
    "        inear = 1\n",
    "\n",
    "    x1 = x[inear]\n",
    "    y1 = y[inear]\n",
    "    z1 = z[inear]\n",
    "\n",
    "    r = np.sqrt(x1**2 + y1**2 + z1**2)\n",
    "\n",
    "    theta = np.arccos(z1/r)\n",
    "    xlat = 90.0 - theta/dtr\n",
    "\n",
    "    #; convert to planetographic latitude\n",
    "    #; xlat = np.arctan(((Re/Rp)**2)*np.tan(xlat*dtr))/dtr\n",
    "\n",
    "    cphi = x1/(r*np.sin(theta))\n",
    "\n",
    "    if cphi > 1.0:\n",
    "        cphi = 1.0\n",
    "    if cphi < -1.0:\n",
    "        cphi = -1.0\n",
    "\n",
    "    phi = np.arccos(cphi)\n",
    "    if y1 < 0.0:\n",
    "        phi = -phi\n",
    "    xlon = phi/dtr        \n",
    "\n",
    "    # Finding aphi, zen, szen - don't mess with these, want to still output them\n",
    "\n",
    "    v1 = np.zeros(3)\n",
    "    v2 = np.zeros(3)\n",
    "    v3 = np.zeros(3)\n",
    "\n",
    "    # v1 is normal vector of point observed\n",
    "    v1[0] = x1/r\n",
    "    v1[1] = y1/r\n",
    "    v1[2] = z1/r\n",
    "\n",
    "    v2[0] = alpha\n",
    "    v2[1] = beta\n",
    "    v2[2] = gamma\n",
    "\n",
    "    summ = 0.0\n",
    "\n",
    "    for i in range(0,3):\n",
    "        summ += v1[i]*v2[i]\n",
    "    zen = np.arccos(summ)/dtr\n",
    "\n",
    "    # Finding aphi\n",
    "\n",
    "    alphasol = np.sin(np.pi/2.0 - latsol*dtr)*np.cos(lonsol*dtr)\n",
    "    betasol =  np.sin(np.pi/2.0 - latsol*dtr)*np.sin(lonsol*dtr)\n",
    "    gammasol = np.cos(np.pi/2 - latsol*dtr)\n",
    "    v3[0]=alphasol\n",
    "    v3[1]=betasol\n",
    "    v3[2]=gammasol\n",
    "\n",
    "    summ = 0.0\n",
    "    for i in range(0,3):\n",
    "        summ += v1[i]*v3[i]\n",
    "    szen = np.arccos(summ)/dtr\n",
    "\n",
    "    cphase = 0.0\n",
    "\n",
    "    for i in range(0,3):\n",
    "        cphase += v2[i]*v3[i]\n",
    "\n",
    "    a = np.cos(zen*dtr)*np.cos(szen*dtr)\n",
    "    b = np.sin(zen*dtr)*np.sin(szen*dtr)\n",
    "\n",
    "    if b == 0.0:\n",
    "        aphi = 180.0\n",
    "    else:\n",
    "        cphi = (cphase-a)/b\n",
    "        aphi = 180.0-np.arccos(cphi)/dtr\n",
    "            \n",
    "    longitude = se_lon-xlon # offset of longitude added to sub-observer longitude\n",
    "    # Remember that system III longitude increases to the west\n",
    "    \n",
    "    #return xlat,longitude\n",
    "    return iflag,xlat,longitude,xlon,zen,szen,aphi\n",
    "    #return xlat,longitude,xlon,zen,szen,aphi\n",
    "\n",
    "projposolar_vec = np.vectorize(projposolar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f322338c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def distance_finder(filename, ut_date='DATE_OBS', ut_time='TIME_OBS', target_name='Jupiter', location_code='568'):\n",
    "    '''\n",
    "    Queries horizons and produces distance in AU. Just copied relevant content from map_maker().\n",
    "    \n",
    "    Input:\n",
    "    filename - string, path+name of fits file to map.    \n",
    "    \n",
    "    optional keyword arguments; defaults are those used by IRTF SpeX Guidedog camera\n",
    "    ut_date - string, the header keyword for UT date of the observation\n",
    "    ut_time - string, the header keyword for the average UT time of the observation\n",
    "    target_name - string, planet being observed. so far just the gas giants are included. Adding more will require including more of the planets and their corresponding Horizons codes in the planets variable.\n",
    "    location_code - string, horizons code of observing location. default = 568, Mauna Kea \n",
    "    \n",
    "    output:\n",
    "    distance to planet in AU\n",
    "    '''\n",
    "    planets = ('599','Jupiter',71492,66854,0.06487),('699','Saturn',60268,54364,0.09796),('799','Uranus',25559,24973,0.0229),('899','Neptune',24766,24342,0.0171)\n",
    "    \n",
    "    # import image\n",
    "    im = fits.open(filename)\n",
    "    header = im[0].header\n",
    "\n",
    "    # based on UT time/date and location of observations, query Horizons for viewing geom info\n",
    "    UT_date = header[ut_date]; UT_time = header[ut_time]; target = target_name\n",
    "    \n",
    "    # Account for fractional seconds in UT_time if it exists. datetime does not like fractional seconds.\n",
    "    if '.' in UT_time:\n",
    "        nofrag, frag = UT_time.split('.')\n",
    "        UT_time = nofrag    \n",
    "\n",
    "    # Query horizons. !! The date/time format for UT_date and UT_time as used in date_obj are specific to the format used by the IRTF.\n",
    "    for k in planets: # find planet code, r_eq, r_pol, obl\n",
    "        if k[1] == target: # assumes target string is exactly correct.\n",
    "            id_number = k[0]; planet_label = k[1]; req = k[2]; rpol = k[3]; obl = k[4]\n",
    "\n",
    "    # calculate julian date\n",
    "    date_obj = datetime.strptime(UT_date+' '+UT_time, '%Y-%m-%d %H:%M:%S') # define datetime object based on UT date and time. Might have issue bc seconds are fractional\n",
    "    date_only_obj = datetime.strptime(UT_date, '%Y-%m-%d')\n",
    "    h = int(date_obj.strftime(\"%H\")); m = int(date_obj.strftime(\"%M\")); s = int(date_obj.strftime(\"%S\"))\n",
    "    dt = timedelta(hours=h, minutes=m, seconds=s)\n",
    "    secs_per_day = 24*60*60    # hours * mins * secs\n",
    "    jd_time_fraction = dt.total_seconds()/secs_per_day # fraction of day time, to add to date\n",
    "\n",
    "    jd = date_only_obj.toordinal() + 1721424.5 + jd_time_fraction # based on https://stackoverflow.com/questions/13943062/extract-day-of-year-and-julian-day-from-a-string-date\n",
    "\n",
    "    # put ephimerides into a table\n",
    "    table = Horizons(id=id_number, location=location_code, epochs=jd).ephemerides()\n",
    "    distance_au = table['r'][0]\n",
    "    \n",
    "    return distance_au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e6fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a3e840",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def map_maker(filename, plot_maps=0,  ut_date='DATE_OBS', ut_time='TIME_OBS', target_name='Jupiter', location_code='568', pixelscale=0.115696):\n",
    "\n",
    "    '''\n",
    "    Program to generate viewing geometry maps. Will first query Horizons for \n",
    "\n",
    "    Inputs\n",
    "    -----------\n",
    "    filename - string, path+name of fits file to map.\n",
    "    plot_maps - 0 or 1, int. on/off switch for plotting diagnostic plots. 0 = no plots, 1 = plots.\n",
    "\n",
    "    optional keyword arguments; defaults are those used by IRTF SpeX Guidedog camera\n",
    "    ut_date - string, the header keyword for UT date of the observation\n",
    "    ut_time - string, the header keyword for the average UT time of the observation\n",
    "    target - string, planet being observed. so far just the gas giants are included. Adding more will require including more of the planets and their corresponding Horizons codes in the planets variable.\n",
    "    location_code - string, horizons code of observing location. default = 568, Mauna Kea\n",
    "    pixelscale - float, pixel scale of instrument in \"/pix. Default is guidedog on IRTF SpeX\n",
    "\n",
    "    Outputs\n",
    "    -----------\n",
    "    iflag - 2D array, map of planet location. 1 where planet is, -1 where planet is not. !! save my self-made iflag map?\n",
    "    latitude_final - 2D array, float, planetocentric latitude. nans everywhere else\n",
    "    longitude_final - 2D array, float, sys III longitude (increases to west). nans everywhere else\n",
    "    xlon - 2D array, longitude offset from meridian. nans everywhere else\n",
    "    zen - 2D array of zenith emission angle. nans everywhere else\n",
    "    szen - 2D array of soalr emission angle. nans everywhere else\n",
    "    aphi - 2D array of azimuth angle. nans everywhere else\n",
    "    data - 2D array of cropped, rotated, and aligned data. nans everywhere else\n",
    "    ob_lon - sub-observer longitude (lcm in later functions)\n",
    "    distance_au - distance to Jupiter in AU from the Earth (needed for spectrum file)\n",
    "    '''\n",
    "\n",
    "    print('Loading image info and querying Horizons...')\n",
    "\n",
    "    #tuples of planets and their Horizons codes. Only gas giants for now, can add terrestrial planets. Horizons code, name, equatorial radius (km), polar radius (km), oblateness. would be nice to be able to pull radii from horizons eventually\n",
    "    planets = ('599','Jupiter',71492,66854,0.06487),('699','Saturn',60268,54364,0.09796),('799','Uranus',25559,24973,0.0229),('899','Neptune',24766,24342,0.0171)\n",
    "\n",
    "    # import image\n",
    "    im = fits.open(filename)\n",
    "    header = im[0].header\n",
    "    data = im[0].data # 'data' variable will be used by pylanetary\n",
    "    if plot_maps == 1:\n",
    "        plt.imshow(data,origin='lower')\n",
    "        plt.title('original data')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # based on UT time/date and location of observations, query Horizons for viewing geom info\n",
    "    UT_date = header[ut_date] \n",
    "    UT_time = header[ut_time]\n",
    "    target = target_name\n",
    "    #target = header[target_keyword] # !! would like to make this query the header, but might generate some issues if target names aren't recorded the same across different datasets. \n",
    "\n",
    "    # Account for fractional seconds in UT_time if it exists. datetime does not like fractional seconds.\n",
    "    if '.' in UT_time:\n",
    "        nofrag, frag = UT_time.split('.')\n",
    "        UT_time = nofrag    \n",
    "\n",
    "    # Query horizons. !! The date/time format for UT_date and UT_time as used in date_obj are specific to the format used by the IRTF.\n",
    "\n",
    "    for k in planets: # find planet code, r_eq, r_pol, obl\n",
    "        if k[1] == target: # assumes target string is exactly correct. might be an issue if there are additional spaces\n",
    "            id_number = k[0]; planet_label = k[1]; req = k[2]; rpol = k[3]; obl = k[4]\n",
    "\n",
    "    # calculate julian date\n",
    "    date_obj = datetime.strptime(UT_date+' '+UT_time, '%Y-%m-%d %H:%M:%S') # define datetime object based on UT date and time. Might have issue bc seconds are fractional\n",
    "    date_only_obj = datetime.strptime(UT_date, '%Y-%m-%d')\n",
    "    h = int(date_obj.strftime(\"%H\"))\n",
    "    m = int(date_obj.strftime(\"%M\"))\n",
    "    s = int(date_obj.strftime(\"%S\"))\n",
    "    dt = timedelta(hours=h, minutes=m, seconds=s)\n",
    "    secs_per_day = 24*60*60    # hours * mins * secs\n",
    "    jd_time_fraction = dt.total_seconds()/secs_per_day # fraction of day time, to add to date\n",
    "\n",
    "    jd = date_only_obj.toordinal() + 1721424.5 + jd_time_fraction # based on https://stackoverflow.com/questions/13943062/extract-day-of-year-and-julian-day-from-a-string-date\n",
    "\n",
    "    # define horizons query\n",
    "    obj = Horizons(id=id_number, location=location_code, epochs=jd)\n",
    "\n",
    "    # put ephimerides into a table\n",
    "    table = obj.ephemerides()\n",
    "    distance_au = table['r'][0]\n",
    "    distance = table['r'][0]*1.496E8 # in AU, convert to km\n",
    "\n",
    "    # define values needed for ModelEllopsoid\n",
    "    ob_lon = table['PDObsLon'][0] #sub-observer longitude, degrees\n",
    "    ob_lat = table['PDObsLat'][0] #sub-observer latitude, degrees, planetocentric\n",
    "    sol_lon = table['PDSunLon'][0] #sub-solar longitude, degrees\n",
    "    sol_lat = table['PDSunLat'][0] #sub-solar latitude, degrees, planetocentric\n",
    "    pixscale_km = np.arctan(pixelscale*(1/206265))*distance\n",
    "    np_ang = table['NPole_ang'][0] #degrees \n",
    "\n",
    "    # save values of values needed for projposolar, outside of eoff and poff. gratiutous but I'm tired\n",
    "    Re = (req/pixscale_km)*pixelscale # equatorial radius in arcsec\n",
    "    epsilon = ob_lat # Sub-observer (planetographic) latitude\n",
    "    latsol = sol_lat # Sub-solar planetocentric latitude\n",
    "    lonsol = sol_lon-ob_lon # longitude difference between sub-solar and sub-observer points\n",
    "\n",
    "    # define offset and rotation values to be used for both data and iflag map rotation\n",
    "    # !! hard coded planet center value keywords for IRTF\n",
    "    cx = im[0].header['cx']; cy = im[0].header['cy']\n",
    "    # x and y center of image\n",
    "    x1 = im[0].header['naxis1']/2; y1 = im[0].header['naxis2']/2\n",
    "    x_offset = int(cx-x1); y_offset = int(cy-y1)\n",
    "    rot_ang = np.copy(np_ang)\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - \n",
    "    # use pylanetary to generate map of where planet is and isn't, Might not need all lines here but keeping anyways to make sure nothing breaks\n",
    "\n",
    "    obs_time = UT_date + ' ' + UT_time\n",
    "    ellipsoid = ModelEllipsoid(ob_lon, ob_lat, pixscale_km, np_ang, req, rpol)\n",
    "    body = Body(target, epoch=obs_time, location=location_code) # I believe this pulls the north pole angle automatically\n",
    "    model = ModelBody(body, pixelscale) # can define the shape here if needed\n",
    "    nav = Nav(data, body, pixelscale)    \n",
    "\n",
    "    # use nav.mu to make iflag map. rotated_mu_map will ultimately become iflag_map\n",
    "    rotated_mu_map = nav.mu\n",
    "\n",
    "    rotated_mu_map[np.where(np.isnan(rotated_mu_map)==True)] = 0 # make nans 0 so ndimage doesn't freak out.\n",
    "    # rotate\n",
    "    rotated_mu_map = ndimage.rotate(rotated_mu_map,rot_ang)\n",
    "\n",
    "    # crop; ! might want to just use the same crop values for both data and iflag centeirng?\n",
    "    # if the x-length of the rotated map is bigger than the original image's x-height, same for y:\n",
    "    if len(rotated_mu_map[0]) > header['naxis1'] and len(rotated_mu_map) > header['naxis2']:\n",
    "        y_crop = int((len(rotated_mu_map)-header['naxis2'])/2)\n",
    "        x_crop = int((len(rotated_mu_map[0])-header['naxis1'])/2)\n",
    "        rotated_mu_map = rotated_mu_map[y_crop:-y_crop,x_crop:-x_crop]\n",
    "\n",
    "\n",
    "    # center, rotate, and crop data\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - \n",
    "    # rotate and crop data after it's been used to orient the iflag map\n",
    "    # rotate and center image \n",
    "    data=np.roll(data,-y_offset,axis=0)\n",
    "    data=np.roll(data,-x_offset,axis=1)\n",
    "    data = ndimage.rotate(data,np_ang)\n",
    "\n",
    "    # crop centered and rotated data; updating values here\n",
    "    x_crop = 0; y_crop = 0\n",
    "\n",
    "    if len(data[0]) > header['naxis1'] and len(data) > header['naxis2']:\n",
    "        print('cropping data')\n",
    "        y_crop = int((len(data)-header['naxis2'])/2)\n",
    "        x_crop = int((len(data[0])-header['naxis1'])/2)\n",
    "        data = data[y_crop:-y_crop,x_crop:-x_crop]\n",
    "\n",
    "    # update rotation angle and offset since data has been rotated and aligned\n",
    "    np_ang = 0\n",
    "    x_offset = 0; y_offset = 0\n",
    "\n",
    "\n",
    "    if plot_maps == 1:\n",
    "        print('! Check here to make sure meridian isnt crossed by GRS or moon, which can bias I/F cal')\n",
    "        # plot data and cross-hairs\n",
    "        x_half = len(data[0])/2\n",
    "        y_half = len(data)/2\n",
    "        plt.imshow(data,origin='lower')\n",
    "        plt.plot([x_half,x_half],[0,len(data)-1],color='red')\n",
    "        plt.plot([0,len(data[0])-1],[y_half,y_half],color='red')\n",
    "        plt.title('centered rotated and cropped data')\n",
    "        plt.show()\n",
    "\n",
    "    # make sure iflag map and data have the same dimensions (otherwise will have issues mapping arrays later)\n",
    "    if len(data) != len(rotated_mu_map) or len(data[0]) != len(rotated_mu_map[0]):\n",
    "        print('! size of iflag map and data are not equal !')\n",
    "        print('len(data):',len(data),'rotated_mu_map:',len(rotated_mu_map))\n",
    "        print('len(data[0]):',len(data[0]),'rotated_mu_map[0]:',len(rotated_mu_map[0]))\n",
    "        # if wrong numbers but ~1 pixel off, it's probably fine (if we're looking at Jupiter with plenty of pixels to spare)\n",
    "\n",
    "    # use the rotated mu map to finish making iflag_map.\n",
    "    # since using nd image rotate on 0's, very close to 0, rounding down. !!! very janky, likely a more elegant way to do this. \n",
    "    iflag_map = np.copy(rotated_mu_map)\n",
    "    iflag_map = np.round(iflag_map,1)\n",
    "    iflag_map[np.where(iflag_map==0)] = 0\n",
    "    iflag_map[np.where(iflag_map!=0)] = 1\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - -\n",
    "    # find eoff and poff, equatorial and polar offset needed for projposolar. Much of this is originally from working with tricky rotation so mostly unnecessart now, but leaving in in case we want to implement it later.\n",
    "\n",
    "    # finding center point based on data array (should be same as x1 and y1 earlier?)\n",
    "    x_center = len(data)/2\n",
    "    y_center = len(data[0])/2\n",
    "\n",
    "    # don't need but might be helpful for testing\n",
    "    unrot_x = np.where(iflag_map==1)[0]\n",
    "    unrot_y = np.where(iflag_map==1)[1]\n",
    "\n",
    "    # list of coordinates where the planet is. Shifted to have origin at middle of image/planet\n",
    "    x = unrot_x-x_center\n",
    "    y = unrot_y-y_center\n",
    "\n",
    "    np_ang_radians = np_ang*0.0174533 # rad/deg # in case still need to rotate. should be 0 if data already rotated\n",
    "\n",
    "    # flipped these bc lat and long maps where backwards\n",
    "    eoff = -x*np.sin(np_ang_radians)+y*np.cos(np_ang_radians)\n",
    "    poff = x*np.cos(np_ang_radians)+y*np.sin(np_ang_radians)\n",
    "\n",
    "    # translate the pixel offsets to arcsec offsets    \n",
    "    eoff*=pixelscale \n",
    "    poff*=pixelscale\n",
    "\n",
    "    # map eoff and poff values onto planet pixel locations using iflag\n",
    "    eoff_final = np.copy(iflag_map)\n",
    "    poff_final = np.copy(iflag_map)\n",
    "\n",
    "    eoff_final[np.where(iflag_map==1)] = eoff\n",
    "    poff_final[np.where(iflag_map==1)] = poff\n",
    "\n",
    "    eoff_final[np.where(iflag_map!=1)] = np.nan\n",
    "    poff_final[np.where(iflag_map!=1)] = np.nan\n",
    "\n",
    "    if plot_maps == 1: # commented out bc not really necessary. Helpful for assessing issues though\n",
    "        plt.imshow(eoff_final,origin='lower')\n",
    "        plt.title('equatorial offset in arcsec')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        plt.imshow(poff_final,origin='lower')\n",
    "        plt.colorbar()\n",
    "        plt.title('polar offset in arcsec')\n",
    "        plt.show()\n",
    "\n",
    "    # run projposolar, usng iflag map to find location of planet pixels\n",
    "    print('Generating maps...(ignore following errors, they are accounted for w/ iflag map)')\n",
    "\n",
    "    # will get mad if passed nans\n",
    "    # generate maps of all the following values:\n",
    "    iflag,latitude_final,longitude_final,xlon,zen,szen,aphi = projposolar_vec(Re,obl,epsilon,latsol,lonsol,ob_lon,eoff_final,poff_final)\n",
    "\n",
    "    # flip maps along vertical axis (probably more elegant way to do this)\n",
    "    szen=np.flip(szen,axis=1)\n",
    "    aphi=np.flip(aphi,axis=1)\n",
    "\n",
    "    if plot_maps == 1:\n",
    "        plt.imshow(data,origin='lower')\n",
    "        plt.colorbar(label='Data')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.imshow(iflag,origin='lower')\n",
    "        plt.colorbar(label='iflag')\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(rotated_mu_map,origin='lower')\n",
    "        plt.colorbar(label='mu')\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(latitude_final,origin='lower')\n",
    "        plt.colorbar(label='Latitude')\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(longitude_final,origin='lower')\n",
    "        plt.colorbar(label='Longitude (Sys III)')\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(zen,origin='lower')\n",
    "        plt.colorbar(label='Zenith emission angle')\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(szen,origin='lower')\n",
    "        plt.colorbar(label='Solar zenith angle')\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(aphi,origin='lower')\n",
    "        plt.colorbar(label='Azimuth angle')\n",
    "        plt.show()\n",
    "\n",
    "    return iflag,latitude_final,longitude_final,xlon,zen,szen,aphi,data,ob_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd091d42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def spex_wavelengths(filename):\n",
    "    '''\n",
    "    Routine to find the correct wavelength for a SpeX Guidedog image given certain OS filter and G filter combos.\n",
    "    wavelength: midpoint originally from table at http://irtfweb.ifa.hawaii.edu/~spex/work/filters/filters.html (but link seems to be broken now)\n",
    "    \n",
    "    input\n",
    "    -filename: string, path+name of fits file to read. presumption is that this is a guidedog image.\n",
    "    \n",
    "    output\n",
    "    -Wavelength float. mircons\n",
    "    will return error if no matching wavelength is found\n",
    "    '''\n",
    "    wavelength = -1\n",
    "    \n",
    "    im = fits.open(filename)\n",
    "    header = im[0].header\n",
    "    osfilt = header['OSF']\n",
    "    gfilt = header['GFLT']\n",
    "    \n",
    "    if gfilt == 'Open':\n",
    "        # define list of tuples\n",
    "        wavelength_tuple = (('Open',0.00),('Blank',0.00),('PK-50',2.5), ('Opt',2.5),  ('0.1',0.1), ('Long4',5.2), ('Long5',3.86), ('Long6',3.33), ('Short3',2.22), ('Short4',1.63), ('Short5',1.27), ('Short6',1.15), ('Short7',.95), ('CH4_l' ,1.69), ('CH4_s',1.58))\n",
    "        for j in wavelength_tuple:\n",
    "            if j[0] == osfilt:\n",
    "                wavelength = j[1]\n",
    "                return wavelength\n",
    "        \n",
    "    elif gfilt == 'H' and osfilt == 'CH4_s':\n",
    "        wavelength = 1.58\n",
    "        return wavelength\n",
    "    # special case added to what was originally in idl program\n",
    "    elif gfilt == 'J' and osfilt == 'CH4_s':\n",
    "        wavelength = 1.58\n",
    "        return wavelength\n",
    "    elif gfilt == 'H' and osfilt == 'CH4_l':\n",
    "        wavelength = 1.69\n",
    "        return wavelength\n",
    "        \n",
    "    else:\n",
    "        # define list of tuplesso i\n",
    "        wavelength_tuple = (('Open',0.00), ('Blank',0.00), ('Z',1.00),('J',1.215), ('H',1.654), ('K',2.23), (\"L'\",3.80), ('Lp',3.80), (\"M'\",4.76) ,('FeII',1.64), ('H2',2.12), ('Bry',2.16), ('contK',2.26), ('CO',2.29), (\"M'+ND1\",4.76), ('3.417',3.417), ('3.454',3.454), ('5.1',5.100))\n",
    "        for j in wavelength_tuple:\n",
    "            if j[0] == gfilt:\n",
    "                wavelength = j[1]\n",
    "                return wavelength\n",
    "        \n",
    "    if wavelength == -1:\n",
    "        print('Error! No matching wavelength found for filter combo by spex_wavelengths()')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da21b4e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def map_saver(dictionary,key,map_save_location,wl):\n",
    "    '''\n",
    "    just saves the map of mapvalue (e.g. aphi, zen, etc.) to a file. Making this a routine to avoid copying the same 3 lines over and over    \n",
    "\n",
    "    dictionary - wavelength dictionary of map dictionaries\n",
    "    key - string, label of map, will be used as key to pull that array\n",
    "    map_save_location - string, path of location to save maps\n",
    "    wl - wavelength of dictionary of dictionaries\n",
    "    '''\n",
    "    wl = str(wl)\n",
    "    wl_dict = dictionary[wl]\n",
    "    np.savetxt(map_save_location+key+'_'+str(wl),wl_dict[key+'_'+str(wl)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c0f8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python version of congrid, an idl routine\n",
    "\n",
    "def calibcm(data, iflag, lat, wavelength, meancm_path='/prvt/ilio/EZDisturbance/haze/',plot_maps=0,lat_to_ignore=(0,0)):\n",
    "    '''\n",
    "    Finding I/F scaling factors for each wavelength. adapted from Glenn's calibcm*.pro programs\n",
    "    this requires that you have the meancm* data for each wavelength. I've copied them here and renamed them to correspond directly to the wavelength instead of filter name\n",
    "    !! Add a routine here that looks at all available meancm*.sav files in meancm_path, and then ignores wavelengths that aren't there and reports them with an error message. Probably right at beginning.\n",
    "    \n",
    "    data - 2D array, data.\n",
    "    iflag - 2D iflag map. \n",
    "    lat - lat map\n",
    "    mu - 2D array, mu map. don't need, glenn uses it to find the CM of the image\n",
    "    wavelength - float, wavelength in microns of that file. used to find the right meancm file (right now only able to do 1.58, 1.64, 1.69, 2.12, 2.16, 2.26) \n",
    "    meancm_path - string, path to location of .sav files. default is their location at JPL.\n",
    "    lat_to_ignore - tuple of ints representing latitudes corresponding to region to ignore when calculating I/F, for cases w/ moon, GRS, etc. in the way. smaller/negative latitude needs to be first. Needs to be INTS!!\n",
    "    '''\n",
    "    wavelstr = str(wavelength)\n",
    "    \n",
    "    meancm_name= 'meancm'+wavelstr[0]+wavelstr[-2:]+'.sav'\n",
    "    meancm = io.readsav(meancm_path+meancm_name)\n",
    "    calibstdcm = meancm['meancm']\n",
    "    \n",
    "    lat_list = np.arange(-90,91) # list of pos/neg latitudes\n",
    "    \n",
    "    #if plot_maps == 1:\n",
    "    #    plt.plot(calibstdcm[10:170],label='Mean historic I/F value')\n",
    "    #    plt.ylabel('I/F')\n",
    "    #    plt.legend()\n",
    "    #    plt.show()\n",
    "    \n",
    "    # take the sum of the mean cm values within -80 to +80 deg\n",
    "    if lat_to_ignore != (0,0): \n",
    "        # sum up latitudes outside of the region to ignore\n",
    "        lat_int_0 = np.where(lat_list==lat_to_ignore[0])[0][0]\n",
    "        lat_int_1 = np.where(lat_list==lat_to_ignore[1])[0][0]\n",
    "        calibstd = np.sum(calibstdcm[10:lat_int_0])+np.sum(calibstdcm[lat_int_1:170])\n",
    "    else:\n",
    "        calibstd = np.sum(calibstdcm[10:170])\n",
    "    \n",
    "    # pull cm of image. simpler than glenn's version bc its already centered\n",
    "    xcen = int(len(data)/2)\n",
    "    ycen = int(len(data[xcen])/2)\n",
    "\n",
    "    cmscan = data[:,xcen] # if using python version of congrid, pull 3 columns around center bc FREAKS otherwise\n",
    "    lat_region = lat[:,xcen]\n",
    "    \n",
    "    # plot cmscan with latitude to find latitudes we might want to ignore\n",
    "    if plot_maps == 1:\n",
    "        plt.plot(lat_region,cmscan,label='Raw pixel values from data')\n",
    "        if lat_to_ignore != (0,0):\n",
    "            plt.plot([lat_to_ignore[0],lat_to_ignore[0]],[np.min(cmscan),np.max(cmscan)],color='red', label='region to exclude')\n",
    "            plt.plot([lat_to_ignore[1],lat_to_ignore[1]],[np.min(cmscan),np.max(cmscan)],color='red')\n",
    "        plt.ylabel('Raw data pixel values')\n",
    "        plt.xlabel('Latitude')\n",
    "        plt.legend()\n",
    "        plt.xlim(-80,80)\n",
    "        plt.show()\n",
    "\n",
    "    # crop off nans\n",
    "    #cmscan = cmscan[np.where(iflag[:,int(len(data)/2)] != -1)]\n",
    "    if lat_to_ignore != (0,0):\n",
    "        first_half = np.where((lat[:,int(len(data)/2)] > -80)&(lat[:,int(len(data)/2)] < lat_to_ignore[0]))\n",
    "        second_half = np.where((lat[:,int(len(data)/2)] > lat_to_ignore[1])&(lat[:,int(len(data)/2)] < +80))\n",
    "        where_array = np.concatenate((first_half,second_half),axis=1)\n",
    "        cmscan = cmscan[where_array][0]\n",
    "    else: # if not ignoring any laittudes, pull the whole region between N/S 80 deg\n",
    "        cmscan = cmscan[np.where((lat[:,int(len(data)/2)] > -80)&(lat[:,int(len(data)/2)] < +80))]\n",
    "\n",
    "    # calculate new grid to plot the average values w/ the data\n",
    "    interval = len(cmscan)/len(calibstdcm[10:170])\n",
    "    jj = 0\n",
    "    regrid_x = []\n",
    "    for i in range(0,len(calibstdcm[10:170])):\n",
    "        regrid_x.append(interval*jj)\n",
    "        jj+=1\n",
    "    \n",
    "    # take the sum between 80 and 80 deg lat\n",
    "    cmtotal = np.sum(cmscan)\n",
    "    \n",
    "    print(calibstd)\n",
    "    print(cmtotal)\n",
    "    \n",
    "    newscale = calibstd/cmtotal # scale factor to be applied to data\n",
    "    \n",
    "    if plot_maps == 1:\n",
    "        plt.plot(regrid_x,calibstdcm[10:170],label='Mean I/F value') # regrid_x - the lat array corresponding to I/F values after cropping out latitude\n",
    "        plt.plot(cmscan*newscale,label='Scaled data pixel values')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return newscale #, np.asfarray(cmscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf5d6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add error_finder() here. Run it within spextraction_images and return another array w/ error for each average spectrum. maybe after I/F scaling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de29c227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def spextraction_images(format_file, data_list, map_save_location, wavelength_keyword='OSF', plot_maps=0, map_load=0, IF_scale=0, meancm_path='/prvt/ilio/EZDisturbance/haze/', alternate_error_region=0):\n",
    "    # !!! change map_save_location\n",
    "    '''    \n",
    "    Extract spectra based on user input, using maps generated by map_maker(). This version is for extracting spectra from images. Maybe make other versions later?\n",
    "    Added calibcm() for dps. only takes 6 wavelengths at the moment.\n",
    "    \n",
    "    --------------------\n",
    "    Input:\n",
    "    format_file - string, path+name of input file. contains code for what kind of spectra to make (1=equal mu bins within a given latitude/longitude bin). For now, only 1 format but will change that.\n",
    "    data_list - input file containing paths+names of images to extract spectra from. Proabbly just want one image per wavelength and grouped together in time (up to the user how to do that). Will sort by wavelength here so no need to do that in the input file. Will assume that the list contains images of the same target. \n",
    "    plot_maps - on/off switch, int. Will plot maps\n",
    "    \n",
    "    saving/loading things: take advantage of this! otherwise takes a while to make maps new every time.\n",
    "    !! bug: change map load/save to a single variable to avoid a user having both map_save=1 and load_maps=1 which will break.\n",
    "    map_load - int, on/off switch. 0, will save maps to path defined by map_save_location. 1, will load maps from that location.\n",
    "    map_save_location - path to location of saved maps. \n",
    "    IF_scale = int, 1 or 0. 1, will use python version of calibcm to scale the data to I/F\n",
    "    meancm_path - string, path to location of .sav files. default is their location at JPL.\n",
    "    alternate_error_region - int, 0 or 1. if the default region selected to estimate error doesn't work (e.g., a moon in the way), set to 1 and use a region on the other side of the planet.\n",
    "\n",
    "    \n",
    "    output:\n",
    "    arrays for each image/wavelength\n",
    "    error_estimate - assumed to be the square of the variance of the background. potentially a better way to do this.\n",
    "    \n",
    "    # !! lots of comments with !! need attention. didn't bother to write htem here as to-do items\n",
    "    but lcm is acting real weird. since we're doing meridian\n",
    "    # !! error_estimate is hard-coded to a region of the image to find the stdev.\n",
    "    '''\n",
    "    \n",
    "    ignore_feature = 0 # on/off switch, special case if there's a feature in the way while taking a zonal measurement. E.g., if the GRS is in the way in the STrZ and you just want STrZ spectra. If 1, manually enter longitudes at the bottom of this routine; !! find a better way to do this. seems too special a case though to include it as input?\n",
    "\n",
    "    \n",
    "    datalist = np.loadtxt(data_list,dtype=str) # load list of images\n",
    "    guidedog_wavelengths = () # list of tuples that correspond to the filters used and corresponding wavelengths. OSF, GFLT, and then WL # !! might not use this\n",
    "    \n",
    "    # sort images by wavelength\n",
    "    wl = []\n",
    "    image = []\n",
    "    if wavelength_keyword == 'OSF': # if spex guidedog images\n",
    "        for j in datalist:\n",
    "            wl.append(spex_wavelengths(j))\n",
    "            image.append(j)\n",
    "    else:\n",
    "        for j in datalist:\n",
    "            im = fits.open(j)\n",
    "            wl.append(im[0].header[wavelength_keyword])\n",
    "            image.append(j)\n",
    "    # sort images based on wavelength\n",
    "    wl_sorted_images = [x for _,x in sorted(zip(wl,image))]\n",
    "    wavelengths_sorted = sorted(wl)\n",
    "    \n",
    "    print(wavelengths_sorted)\n",
    "            \n",
    "    \n",
    "    # - - - - - - - - - - - Map saving/loading\n",
    "    \n",
    "    # make maps for each image/wavelength. Is there a way to write this avoid making new maps every time? save them somewhere with the obs_time in the file name, and we can check to see if it's been saved somewhere already.\n",
    "    map_dict = {}\n",
    "    for i in range(0,len(wl_sorted_images)):\n",
    "        print('Arranging map info for',str(wavelengths_sorted[i]))\n",
    "        if map_load == 1: # load maps\n",
    "            map_dict_dummy = {}\n",
    "            # load each map for that wavelength and save it to the dictionary\n",
    "            iflag = np.loadtxt(map_save_location+'iflag_'+str(wavelengths_sorted[i]))\n",
    "            latitude_final = np.loadtxt(map_save_location+'latitude_final_'+str(wavelengths_sorted[i]))\n",
    "            longitude_final = np.loadtxt(map_save_location+'longitude_final_'+str(wavelengths_sorted[i]))\n",
    "            zen = np.loadtxt(map_save_location+'zen_'+str(wavelengths_sorted[i]))\n",
    "            szen = np.loadtxt(map_save_location+'szen_'+str(wavelengths_sorted[i]))\n",
    "            aphi = np.loadtxt(map_save_location+'aphi_'+str(wavelengths_sorted[i]))\n",
    "            mu = np.loadtxt(map_save_location+'mu_'+str(wavelengths_sorted[i]))\n",
    "            data = np.loadtxt(map_save_location+'data_'+str(wavelengths_sorted[i]))\n",
    "            lcm = np.loadtxt(map_save_location+'lcm_'+str(wavelengths_sorted[i]))\n",
    "            \n",
    "            map_dict_dummy['iflag_'+str(wavelengths_sorted[i])] = iflag\n",
    "            map_dict_dummy['latitude_final_'+str(wavelengths_sorted[i])] = latitude_final\n",
    "            map_dict_dummy['longitude_final_'+str(wavelengths_sorted[i])] = longitude_final \n",
    "            map_dict_dummy['zen_'+str(wavelengths_sorted[i])] = zen \n",
    "            map_dict_dummy['szen_'+str(wavelengths_sorted[i])] = szen\n",
    "            map_dict_dummy['aphi_'+str(wavelengths_sorted[i])] = aphi\n",
    "            map_dict_dummy['mu_'+str(wavelengths_sorted[i])] = mu\n",
    "            map_dict_dummy['data_'+str(wavelengths_sorted[i])] = data\n",
    "            map_dict_dummy['lcm_'+str(wavelengths_sorted[i])] = lcm\n",
    "            map_dict[str(wavelengths_sorted[i])] = map_dict_dummy\n",
    "            \n",
    "        elif map_load == 0: # otherwise, make a dictionary of maps for that wavelength to save them to\n",
    "            map_dict_dummy = {}\n",
    "            iflag,latitude_final,longitude_final,xlon,zen,szen,aphi,data,lcm = map_maker(wl_sorted_images[i],plot_maps)\n",
    "                        \n",
    "            map_dict_dummy['iflag_'+str(wavelengths_sorted[i])] = iflag\n",
    "            map_dict_dummy['latitude_final_'+str(wavelengths_sorted[i])] = latitude_final\n",
    "            map_dict_dummy['longitude_final_'+str(wavelengths_sorted[i])] = longitude_final \n",
    "            map_dict_dummy['zen_'+str(wavelengths_sorted[i])] = zen \n",
    "            map_dict_dummy['szen_'+str(wavelengths_sorted[i])] = szen\n",
    "            map_dict_dummy['aphi_'+str(wavelengths_sorted[i])] = aphi\n",
    "            map_dict_dummy['mu_'+str(wavelengths_sorted[i])] = np.cos(zen*0.0174533)\n",
    "            map_dict_dummy['data_'+str(wavelengths_sorted[i])] = data\n",
    "            # code is getting mad that lcm isn't an array. i'm tired so here is the bad fix\n",
    "            lcm_array = np.copy(data)\n",
    "            lcm_array[:,:] = lcm\n",
    "            map_dict_dummy['lcm_'+str(wavelengths_sorted[i])] = lcm_array\n",
    "            map_dict[str(wavelengths_sorted[i])] = map_dict_dummy            \n",
    "            \n",
    "            print('Saving maps at '+str(wavelengths_sorted[i])+'...')\n",
    "            map_saver(map_dict,'iflag',map_save_location,wavelengths_sorted[i])\n",
    "            map_saver(map_dict,'latitude_final',map_save_location,wavelengths_sorted[i])\n",
    "            map_saver(map_dict,'longitude_final',map_save_location,wavelengths_sorted[i])\n",
    "            map_saver(map_dict,'zen',map_save_location,wavelengths_sorted[i])\n",
    "            map_saver(map_dict,'szen',map_save_location,wavelengths_sorted[i])\n",
    "            map_saver(map_dict,'aphi',map_save_location,wavelengths_sorted[i])\n",
    "            map_saver(map_dict,'mu',map_save_location,wavelengths_sorted[i])\n",
    "            map_saver(map_dict,'data',map_save_location,wavelengths_sorted[i])\n",
    "            map_saver(map_dict,'lcm',map_save_location,wavelengths_sorted[i])\n",
    "            # if normal file.write() stuff works in map_saver: map_saver(iflag,map_save_location,wavelengths_sorted[i])\n",
    "    \n",
    "    # - - - - - - - - - - - - - - - - - - - - - Set up regions from which to extract spectra \n",
    "    \n",
    "    # open input file and determine type of spectrum to make\n",
    "    with open(format_file,'r') as f:\n",
    "        all_data=[x.split() for x in f.readlines()]\n",
    "        spec_type = int(all_data[0][0]) # find and define\n",
    "        if spec_type == 1:\n",
    "            # if mu-binned lat spectrum, find other requirements:\n",
    "            lat_min = float(all_data[1][0])\n",
    "            lat_max = float(all_data[1][1])\n",
    "            mu_min = float(all_data[2][0])\n",
    "            n_bins = int(all_data[3][0])\n",
    "            meridian_switch = float(all_data[4][0])\n",
    "            w_mu = float(all_data[5][0])\n",
    "            \n",
    "        elif spec_type == 2:\n",
    "            # 2 - along a range of latitudes and between mu_min, divide that region into the given number of bins in mu-space\n",
    "            # if mu-binned lat spectrum, find other requirements:\n",
    "            lat_min = float(all_data[1][0])\n",
    "            lat_max = float(all_data[1][1])\n",
    "            mu_min = float(all_data[2][0])\n",
    "            n_bins = int(all_data[3][0])\n",
    "            meridian_switch = float(all_data[4][0])\n",
    "            w_mu = float(all_data[5][0]) # !! does the way I coded this assume 0 cross-over.\n",
    "            \n",
    "        # add more if statements w/ different spectral types here later\n",
    "    \n",
    "    \n",
    "    # # find mu_max from an example map. Does this need to be done at each wavelength? probably fine.\n",
    "    wl_dummy = wavelengths_sorted[0]\n",
    "    mu_map = map_dict[str(wl_dummy)]['mu_'+str(wl_dummy)]\n",
    "    lat_map = map_dict[str(wl_dummy)]['latitude_final_'+str(wl_dummy)]\n",
    "    data_map = map_dict[str(wl_dummy)]['data_'+str(wl_dummy)]\n",
    "    lcm_map = map_dict[str(wl_dummy)]['lcm_'+str(wl_dummy)] # added for DPS, need this for defining the side of the meridian\n",
    "\n",
    "    if spec_type == 1 or spec_type == 2: # if taking a latitudinal slice\n",
    "        coordinates = np.where((lat_map<lat_max) & (lat_map>lat_min))\n",
    "        \n",
    "        mu_max = np.max(mu_map[coordinates])\n",
    "        print('mu_max = ',mu_max)\n",
    "        \n",
    "    #elif spec_type == 3:\n",
    "        # add other options here later if need different coordinates for different extraction regions\n",
    "        # add definition of mu_max unique to this extraction region if needed here\n",
    "    \n",
    "    if plot_maps == 1:\n",
    "        # !! this plotting routine might be particular to the lat slice spectra\n",
    "        new=np.copy(mu_map); new2=np.copy(mu_map); data_test = np.copy(data_map); data_test2 = np.copy(data_map)\n",
    "\n",
    "        new[coordinates] = 5000\n",
    "        new_coordinates = np.where(new<5000) \n",
    "\n",
    "        data_test[coordinates] = np.nan # blank out data we're sampling\n",
    "        data_test2[new_coordinates] = np.nan # show only data we're sampling. Looks smaller but don't worry, will be fine\n",
    "\n",
    "        print('Region of spectral extraction:')\n",
    "        plt.imshow(new,origin='lower')\n",
    "        plt.show()\n",
    "        plt.imshow(data_test,origin='lower')\n",
    "        plt.show()\n",
    "        plt.imshow(data_test2,origin='lower') # this seems very small for some reason?\n",
    "        plt.show()\n",
    "        plt.imshow(mu_map,origin='lower')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "    # - - - - - - - - - - - - - - - - - - - - -  based on spectral extraction method being used, define parameters for extracting spectra \n",
    "    \n",
    "    # define relevant info about spectral extraction, depending on spectral type in input file\n",
    "    if spec_type == 1:\n",
    "        d_mu = ((mu_max-mu_min)/n_bins) # size of mu bin # !! finding bin size this way assumes we're crossing a mu_max of 1.0. If the latitudes are much different, we need to calculate what mu_max should be. \n",
    "        print('Size of mu bin:',d_mu)\n",
    "        \n",
    "    elif spec_type == 2:\n",
    "        # spec_type = 2: given mu_min and number of bins desired, seperate the given mu_min/latitude rectangle into that many bins. Have to first find mu_max. \n",
    "        d_mu = (2*(mu_max-mu_min))/n_bins # size of each bin\n",
    "        print('Size of mu bin for spec_type=2:',d_mu)\n",
    "        \n",
    "    #elif spec_type == 3:\n",
    "    #    continue # add other spectral types here\n",
    "\n",
    "    # define bins w/ the size of bin (d_mu) in mu-space. not relevant yet\n",
    "    #d_mu = 0.1 # example, if defining by size of bin\n",
    "    \n",
    "    '''\n",
    "    For defining bins w/ size of mu bin and not number of bins\n",
    "    mu_max = 1.0\n",
    "    w_mu = 0.1\n",
    "    n_mu = int((mu_max-mu_min)/d_mu)\n",
    "    n_bins = n_mu*2+1\n",
    "    '''\n",
    "    #n_bins = 2\n",
    "    \n",
    "    # - - - - - - - - - - - - - - - - - - - - - Extract spectra\n",
    "    \n",
    "    n_wavel = len(wavelengths_sorted) # number of wavelengths\n",
    "    wavel_spxs = np.copy(wavelengths_sorted) # just copying this instead of renaming bc lazy\n",
    "    \n",
    "    # utilize dictionaries to save spectra and corresponding geometry values for each wavelength.\n",
    "    # make empty arrays for binning section. columns = bins, rows = wavelengths\n",
    "    extracted_spectrum_ave = np.zeros((n_bins,n_wavel))\n",
    "    extracted_lat_ave = np.zeros((n_bins,n_wavel))\n",
    "    extracted_long_ave = np.zeros((n_bins,n_wavel))\n",
    "    extracted_emiss_ave = np.zeros((n_bins,n_wavel))\n",
    "    extracted_solar_ave = np.zeros((n_bins,n_wavel))\n",
    "    extracted_azi_ave = np.zeros((n_bins,n_wavel))\n",
    "    error_estimate = np.zeros((1,n_wavel))\n",
    "\n",
    "    for i_wavel in range(0,len(wavel_spxs)):\n",
    "        \n",
    "        print('Extracting spectra for',wavel_spxs[i_wavel],'microns...')\n",
    "        \n",
    "        # pull maps for this wavelength\n",
    "        # !!! just raw data here for spxs. make sure to convert to physical units later\n",
    "        spxs = map_dict[str(wavel_spxs[i_wavel])]['data_'+str(wavel_spxs[i_wavel])]\n",
    "        mu_spxs = map_dict[str(wavel_spxs[i_wavel])]['mu_'+str(wavel_spxs[i_wavel])]\n",
    "        lat_spxs = map_dict[str(wavel_spxs[i_wavel])]['latitude_final_'+str(wavel_spxs[i_wavel])]\n",
    "        long_spxs = map_dict[str(wavel_spxs[i_wavel])]['longitude_final_'+str(wavel_spxs[i_wavel])]\n",
    "        solar_spxs = map_dict[str(wavel_spxs[i_wavel])]['szen_'+str(wavel_spxs[i_wavel])]\n",
    "        zen_spxs = map_dict[str(wavel_spxs[i_wavel])]['zen_'+str(wavel_spxs[i_wavel])]\n",
    "        azimuth_spxs = map_dict[str(wavel_spxs[i_wavel])]['aphi_'+str(wavel_spxs[i_wavel])]\n",
    "        lcm_spxs = map_dict[str(wavel_spxs[i_wavel])]['lcm_'+str(wavel_spxs[i_wavel])] \n",
    "        iflag_spxs = map_dict[str(wavel_spxs[i_wavel])]['iflag_'+str(wavel_spxs[i_wavel])] \n",
    "\n",
    "        # once lists are loaded, extract spectra\n",
    "        wavel = wavel_spxs[i_wavel]        \n",
    "        \n",
    "        # - - - - - - - - - - - - - - - - - - - - - pull error value from background variance\n",
    "        \n",
    "        # test error before scaling !!\n",
    "        # Error estimate:\n",
    "        # units of extracted error value will account for IF_scale being turned on or off. \n",
    "        max_iflag_y = np.max(np.where(iflag_spxs==1)[0]); max_iflag_x = np.max(np.where(iflag_spxs==1)[1])\n",
    "        min_iflag_y = np.min(np.where(iflag_spxs==1)[0]); min_iflag_x = np.min(np.where(iflag_spxs==1)[1])\n",
    "\n",
    "        if alternate_error_region == 1:\n",
    "            # if error region doesn't look good, use upper right\n",
    "            error_estimate[0,i_wavel] = np.std(spxs[(max_iflag_x-20):max_iflag_x, (max_iflag_y-20):max_iflag_y])\n",
    "            if plot_maps == 1:\n",
    "                plt.imshow(spxs[(max_iflag_x-20):max_iflag_x, (max_iflag_y-20):max_iflag_y],origin='lower')\n",
    "                print('Error estimate:',error_estimate[0,i_wavel])\n",
    "                plt.colorbar()\n",
    "                plt.title('Error region')\n",
    "                plt.show()\n",
    "            \n",
    "        else:\n",
    "            # otherwise, use lower left\n",
    "            error_estimate[0,i_wavel] = np.std(spxs[min_iflag_x:(min_iflag_x+20), min_iflag_y:(min_iflag_y+20)\n",
    "])\n",
    "            if plot_maps == 1:\n",
    "                plt.imshow(spxs[min_iflag_x:(min_iflag_x+20), min_iflag_y:(min_iflag_y+20)],origin='lower')\n",
    "                print('Error estimate:',error_estimate[0,i_wavel])\n",
    "                plt.colorbar()\n",
    "                plt.title('Error region')\n",
    "                plt.show()\n",
    "                \n",
    "        # !!!! Something weird happening with I/F scale here\n",
    "        if IF_scale == 1:\n",
    "            # scale to I/F\n",
    "            newscale = calibcm(spxs,iflag_spxs,lat_spxs,wavel,meancm_path,plot_maps) # !! assuming default meancm_path.\n",
    "            print('scaling factor for I/F from calibcm():',newscale)\n",
    "            spxs *= newscale\n",
    "            error_estimate[0,i_wavel] = newscale*(error_estimate[0,i_wavel]**2)\n",
    "            if plot_maps == 1:\n",
    "                plt.imshow(spxs,origin='lower')\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "            \n",
    "        else:\n",
    "            error_estimate[0,i_wavel] = error_estimate[0,i_wavel]**2\n",
    "        \n",
    "        \n",
    "        # h = 0 # bin index, for assigning values in arrays. might need if meridian_switch = 0\n",
    "        \n",
    "        # for DPS:\n",
    "        #mu = [(0.5,0.625),(0.875,1.0)]\n",
    "        #print(mu_min,mu_max)\n",
    "        \n",
    "        east_counter = 0 # index to help w/ indexing on the east side of the meridian\n",
    "        east_of_meridian = 0 # will switch to 1 if on the east side of the meridian. on/off switch for pulling spectra from the correct side of the meridian\n",
    "        \n",
    "        # !! this for loop needs to be under an if statement on spec_type !! For now only loops through bins\n",
    "        for i_mu in range(0,n_bins):\n",
    "            # calculate mu boundaries. could make a list of tuples?\n",
    "            # these calculations don't have w_mu in them anywhere !!\n",
    "            on_meridian = 0 # switches to 1 only once, if odd bins and this one is straddling the meridian\n",
    "            \n",
    "            mu_0 = mu_min+float(d_mu*(i_mu))\n",
    "            mu_1 = mu_min+float(d_mu*(i_mu+1))\n",
    "\n",
    "            if  mu_0 < mu_max and mu_1 > mu_max and n_bins%2 == 1:\n",
    "                # if we're on a single mu bin straddling the meridian:\n",
    "                mu_1 = mu_0\n",
    "                on_meridian = 1\n",
    "\n",
    "            elif mu_0 == mu_max and mu_1 > mu_max and n_bins%2 == 0:\n",
    "                # if we're exactly bordering or on the other side of the meridian:\n",
    "                mu_1 = mu_0 # make the max mu value the larger mu variable (bc of np.where statements later)\n",
    "                mu_0 = mu_1-float(d_mu)\n",
    "                east_counter += 1\n",
    "                east_of_meridian = 1\n",
    "\n",
    "            elif mu_0 > mu_max and mu_1 > mu_max and n_bins%2 == 0: \n",
    "                # if we're on the other side of the meridian for even bins:\n",
    "                mu_1 = mu_max-east_counter*d_mu\n",
    "                mu_0 = mu_max-(east_counter+1)*d_mu\n",
    "                east_counter += 1\n",
    "                east_of_meridian = 1\n",
    "\n",
    "            elif mu_0 > mu_max and mu_1 > mu_max and n_bins%2 == 1:\n",
    "                # if we're on the other side of the meridian for odd bins:\n",
    "                mu_1 = mu_max-(east_counter*d_mu+(d_mu/2))\n",
    "                mu_0 = mu_max-((east_counter+1)*d_mu+(d_mu/2))\n",
    "                east_counter += 1\n",
    "                east_of_meridian = 1\n",
    "            \n",
    "            \n",
    "            #mu_0 = mu[i_mu][0]\n",
    "            #mu_1 = mu[i_mu][1]\n",
    "            print('mu_0=', mu_0,'mu_1=',mu_1,', n_bin=',i_mu)\n",
    "                \n",
    "            \n",
    "            # define coordinates depending on bin location relative to meridian\n",
    "            if east_of_meridian == 0 and on_meridian == 0:\n",
    "                # pull spectra from west of meridian, or straddling it\n",
    "                coords = np.where((lat_spxs<lat_max) & (lat_spxs>lat_min) & (mu_spxs>mu_0) & (mu_spxs<mu_1) & (long_spxs > lcm_spxs[0][0]))\n",
    "                \n",
    "            elif on_meridian == 1:\n",
    "                # pull spectrum straddling meridian\n",
    "                coords = np.where((lat_spxs<lat_max) & (lat_spxs>lat_min) & (mu_spxs>mu_0) & (mu_spxs>mu_1))\n",
    "                \n",
    "            elif east_of_meridian == 1 and on_meridian == 0:\n",
    "                # pull spectra from east of meridian\n",
    "                coords = np.where((lat_spxs<lat_max) & (lat_spxs>lat_min) & (mu_spxs>mu_0) & (mu_spxs<mu_1) & (long_spxs < lcm_spxs[0][0]))\n",
    "                \n",
    "            print('Min. longitude of bin:',round(np.min(long_spxs[coords]),3))\n",
    "            print('Max. longitude of bin:',round(np.max(long_spxs[coords]),3))\n",
    "\n",
    "            if plot_maps == 1:\n",
    "                spxs_test = np.copy(spxs)\n",
    "                spxs_test[coords] = np.nan\n",
    "                plt.imshow(spxs_test,origin='lower')\n",
    "                plt.title('extraction region for bin '+str(i_mu))\n",
    "                plt.show()\n",
    "                \n",
    "            # !!!!! just for ignoring GRS or some other feature. Define the lognitudes and will ignore that bin if it overlaps. will have to go and delete those lines in the spx file\n",
    "            bin_ignore = 0\n",
    "            if ignore_feature == 1:\n",
    "                print('Checking if bin should be flagged based on given longitude range...')\n",
    "                long_max = 290.0; long_min = 258.0 # Manually enter longitudes to ignore !!\n",
    "                if True in (long_spxs[coords] > long_min) & (long_spxs[coords] < long_max):\n",
    "                    print('FLAGGING BIN - HIT FORBIDDEN LONGITUDE. DELETE THIS SPECTRAL DATA POINT IN .SPX FILE')\n",
    "                    bin_ignore = 1\n",
    "\n",
    "            # save average of the regions within coords to extracted_* arrays\n",
    "            extracted_spectrum_ave[i_mu,i_wavel] = np.mean(spxs[coords])\n",
    "            extracted_lat_ave[i_mu,i_wavel] = np.mean(lat_spxs[coords])\n",
    "            extracted_long_ave[i_mu,i_wavel] = np.mean(long_spxs[coords])\n",
    "            extracted_emiss_ave[i_mu,i_wavel] = np.mean(zen_spxs[coords])\n",
    "            extracted_solar_ave[i_mu,i_wavel] = np.mean(solar_spxs[coords])\n",
    "            extracted_azi_ave[i_mu,i_wavel] = np.mean(azimuth_spxs[coords])\n",
    "            \n",
    "            print('Average value extracted at '+str(wavel)+': '+str(np.mean(spxs[coords])))\n",
    "            \n",
    "            if bin_ignore == 1:\n",
    "                extracted_spectrum_ave[i_mu,i_wavel] = 1e-10 # arbitrary small number. Will be 0. Will have to go in and delete that spectrum from spx file. !! in future, add something to the file_maker script to do this?\n",
    "            \n",
    "            print('Moving to next bin.... ')\n",
    "            print(' ')\n",
    "\n",
    "        #elif meridian_switch == 0: # deleted earlier if statement for meridian_switch. !! add this eventually.\n",
    "            #print('Emma hasnt coded this yet, do not use')\n",
    "            # here's where lcm would be used. eg as condition for either side of lcm\n",
    "            #return\n",
    " \n",
    "    # add distance-finding routine here. make array w number of bins to assign distances to\n",
    "    # !!!! update mu_max to stay below 1.\n",
    "    \n",
    "    return extracted_spectrum_ave, extracted_lat_ave, extracted_long_ave, extracted_emiss_ave, extracted_solar_ave, extracted_azi_ave, wavelengths_sorted, n_bins, error_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee4a269",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# example test\n",
    "#extracted_spectrum_ave, extracted_lat_ave, extracted_long_ave, extracted_emiss_ave, extracted_solar_ave, extracted_azi_ave, wavelengths_sorted, n_bins, error_estimate = spextraction_images('/Users/emmadahl/Desktop/spextraction/spextraction_input_binned_lat_2.txt', '/Users/emmadahl/Desktop/spextraction/data_list_test.txt', '/Users/emmadahl/Desktop/spextraction/test_maps_2/', map_load=1, plot_maps=1, IF_scale=0, meancm_path='/Users/emmadahl/Desktop/spextraction/meancm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92171634",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# example test - used for debugging\n",
    "#extracted_spectrum_ave, extracted_lat_ave, extracted_long_ave, extracted_emiss_ave, extracted_solar_ave, extracted_azi_ave, wavelengths_sorted, n_bins, error_estimate = spextraction_images('/Users/emmadahl/Desktop/spextraction/spextraction_input_binned_lat.txt',\\\n",
    "#'/Users/emmadahl/Desktop/spextraction/irtf_data_march_2019/2019jun1/input_list', \\\n",
    "#'/Users/emmadahl/Desktop/spextraction/test_maps/', \\\n",
    "#map_load=1, plot_maps=1, IF_scale=0, meancm_path='/Users/emmadahl/Desktop/spextraction/meancm/',alternate_error_region=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf4e56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optional: run this and following cells to make plots of I/F spectra\n",
    "\n",
    "# example test\n",
    "#extracted_spectrum_ave, extracted_lat_ave, extracted_long_ave, extracted_emiss_ave, extracted_solar_ave, extracted_azi_ave, wavelengths_sorted, n_bins, error_estimate = spextraction_images('/Users/emmadahl/Desktop/spextraction/spextraction_input_binned_lat.txt', '/Users/emmadahl/Desktop/spextraction/irtf_data_march_2019/2019jun1/input_list', '/Users/emmadahl/Desktop/spextraction/test_maps_dps/', map_load=1, plot_maps=1, IF_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f02bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c955e8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def IF_plotter(wavelengths_sorted,extracted_spectrum_ave,error_estimate,IF_plot_save_location, error_10_percent=1):\n",
    "    \n",
    "    '''\n",
    "    !! currently commented out below. was messing w/ the I/F scaling.\n",
    "    plot the I/F spectra, assuming there's 2 of them. Assumes things have already been converted to I/F\n",
    "    \n",
    "    wavelengths_sorted - list of wavelengths from spextraction_images\n",
    "    extracted_spectrum_ave - array of average extracted spectra\n",
    "    IF_plot_save_location - string, path+name of location to save pdf image. Make sure filename ends in .pdf\n",
    "    error_10_percent - int, 1 - will assume 10% error; 0, will use error estimate\n",
    "    ''' \n",
    "    \n",
    "    # !!! hard-coded 2 plots\n",
    "\n",
    "    fig, ax = plt.subplots(dpi=200)\n",
    "    fig.set_size_inches(6, 3) # set size\n",
    "    \n",
    "    colors = plt.cm.plasma(np.linspace(0,1,len(extracted_spectrum_ave)))\n",
    "    \n",
    "    for i in range(0,len(extracted_spectrum_ave)):\n",
    "\n",
    "        plt.plot(wavelengths_sorted,extracted_spectrum_ave[i],'.-',color=colors[i])\n",
    "\n",
    "        if error_10_percent == 1:\n",
    "            plt.fill_between(wavelengths_sorted,\\\n",
    "                extracted_spectrum_ave[i]+0.1*extracted_spectrum_ave[i],\\\n",
    "                extracted_spectrum_ave[i]-0.1*extracted_spectrum_ave[i],color=colors[i],alpha=0.2)\n",
    "\n",
    "        else:\n",
    "            plt.fill_between(wavelengths_sorted,\\\n",
    "                             extracted_spectrum_ave[i]+error_estimate[i],\\\n",
    "                             extracted_spectrum_ave[i]-error_estimate[i],color=colors[i],alpha=0.2)\n",
    "\n",
    "    plt.ylabel('I/F')\n",
    "    plt.xlabel('Wavelength (microns)')\n",
    "    #plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(hspace=0.0)\n",
    "    \n",
    "    # save a file containing the I/F spectra !! specific to 2 spectra\n",
    "    file = open('./IF_spectrum','w')\n",
    "    file.write('# Saved automatically by Spextraction; contains I/F spectra \\n')\n",
    "    \n",
    "    if error_10_percent == 1:\n",
    "        file.write('# Wavelength (microns) | I/F, nadir | I/F, limb  | % error \\n') # this header is specific to this setup w/ 2 spectra\n",
    "        for j in range(0,len(wavelengths_sorted)):\n",
    "            file.write(str(wavelengths_sorted[j])+' '+str(extracted_spectrum_ave[1][j])+' '+str(extracted_spectrum_ave[0][j])+' '+str(error_estimate[0][j])+' \\n')\n",
    "        file.close()\n",
    "        \n",
    "    else:\n",
    "        file.write('# Wavelength (microns) | I/F, nadir | I/F, limb  | error estimate from bg \\n') # this header is specific to this setup w/ 2 spectra\n",
    "        for j in range(0,len(wavelengths_sorted)):\n",
    "            file.write(str(wavelengths_sorted[j])+' '+str(round(extracted_spectrum_ave[1][j],6))+' '+str(round(extracted_spectrum_ave[0][j],10))+' '+str(round(error_estimate[0][j],10))+' \\n')\n",
    "        file.close()\n",
    "\n",
    "    plt.savefig(IF_plot_save_location, format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7175a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !! this needs to be a subroutine for spextraction, instead of using it here\n",
    "\n",
    "def spectrum_file_maker(format_file, data_list, map_save_location, specfile, solarfile, meancm_path='/prvt/ilio/EZDisturbance/haze/', wavelength_keyword='OSF', plot_maps=0, map_load=0, IF_scale=0, file_format='Nemesis',IF_plot_save_location='./IF_plot.pdf', save_IF_spec=0, IF_file_save_location='./IF_spectrum', error_percent=0):\n",
    "    '''\n",
    "    Makes a NEMESIS spx file of extracted spectra. Will assume spectra are in units of I/F for conversion, so either need to be cal images that are already in units of I/F or use IF_scale = 1 to scale them here.\n",
    "        \n",
    "    format_file - string, path+name of input file for spectrum bin parameters. contains code for what kind of spectra to make. For now, only 1 format but will change that.\n",
    "    data_list - input file containing paths+names of images from which to extract spectra. Proabbly just want one image per wavelength and grouped together in time (up to the user how to do that). Will sort by wavelength here so no need to do that in the input file. Will assume that the list contains images of the same target. \n",
    "    plot_maps - on/off switch, int. Will plot maps if 1, not if 0.\n",
    "    specfile - string, path+name of spectrum file to be made and saved. \n",
    "    solarfile - file containing solar spectrum. !! need to generate this for guidedog\n",
    "    meancm_path - string, path of location of meancm*.sav files. Default here is where Glenn has them saved.\n",
    "    \n",
    "    map_load - int, on/off switch. 0, will save maps to path defined by map_save_location. 1, will load maps from that location.\n",
    "    map_save_location - path to location of saved maps. \n",
    "    IF_scale = int, 1 or 0. 1, will use python version of calibcm to scale the data to I/F, will automatically make an I/F plot and save it in IF_plot_save_location.\n",
    "    file_format - string, label for desired format of spectrum file. currently only coded for Nemesis spx files.\n",
    "    IF_plot_save_location - string, path+name of location to save I/F plots\n",
    "    save_IF_spec - int, 0 or 1 - 1, will save file containing I/F spectrum; 0, will not\n",
    "    IF_file_save_location - string, path+name to save IF file\n",
    "    error_percent = float, percent error to apply to spectrum. Will look to see if this isn't 0 and then apply it, otherwise will use error estimate from background\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # pull spectra using spextraction_images\n",
    "    extracted_spectrum_ave, extracted_lat_ave, extracted_long_ave, extracted_emiss_ave, extracted_solar_ave, extracted_azi_ave, wavelengths, n_bins, error_estimate = \\\n",
    "    spextraction_images(format_file, data_list, map_save_location, wavelength_keyword=wavelength_keyword, map_load=map_load, plot_maps=plot_maps, IF_scale=IF_scale, meancm_path=meancm_path)\n",
    "    \n",
    "    print(extracted_spectrum_ave)\n",
    "    \n",
    "    #if IF_scale == 1:\n",
    "        # save I/F plots\n",
    "        #IF_plotter(wavelengths,extracted_spectrum_ave,error_estimate,IF_plot_save_location,error_percent)\n",
    "    \n",
    "    # !!!! finds the distance for a single image. Fine assumption as long as time between images isn't huge\n",
    "    distance_file = np.loadtxt(data_list,dtype=str)[0]\n",
    "    r = distance_finder(distance_file)\n",
    "    if file_format == 'Nemesis':\n",
    "        print('For .set file: distance between Sun and planet is '+str(round(r,4))+' AU')\n",
    "\n",
    "    # can probably delete if its being read in. \n",
    "    n_bins = len(extracted_spectrum_ave)\n",
    "    \n",
    "    # - - - - - - - - - - -\n",
    "    \n",
    "    if file_format == 'Nemesis':\n",
    "    \n",
    "        #r = distance_finder() # AU. Might need this to find radiance in loop later\n",
    "        \n",
    "        # add loading of solar file here. Needs to be included as auxiliary file, and same file as in runname.sol\n",
    "        F_sol = np.loadtxt(solarfile) # wavelength, solar radiance/luminosity\n",
    "        # interpolate at our data points. Ideally this will also be convolved with our filter functions\n",
    "        solar_func = interp1d(F_sol[:,0],F_sol[:,1])\n",
    "        F_solar = []\n",
    "        for g in wavelengths:\n",
    "            F_solar.append(solar_func(g))\n",
    "        print('Solar Lum (W/micron) =',F_solar)\n",
    "        \n",
    "        wavel_spxs = np.copy(wavelengths)\n",
    "        \n",
    "        FWHM = 0\n",
    "        LATITUDE = np.mean(extracted_lat_ave) # the average latitude of all spectra in the file\n",
    "        LONGITUDE = np.mean(extracted_long_ave) # the average longitude of all spectra in the file\n",
    "        NGEOM = np.copy(n_bins)\n",
    "        NAV = 1\n",
    "        \n",
    "        file = open(specfile,'w')\n",
    "        # make top header\n",
    "        file.write('     '+str(FWHM)+'  '+str(np.round(LATITUDE,3))+'  '+str(np.round(LONGITUDE,3))+'  '+str(NGEOM )+' \\n')\n",
    "\n",
    "        # for spectra headers\n",
    "        NCONV = len(wavelengths)\n",
    "        NAV = 1\n",
    "\n",
    "        for p in range(0,n_bins):\n",
    "            \n",
    "            spec = np.copy(extracted_spectrum_ave)\n",
    "            \n",
    "            # define header for that mu bin\n",
    "            FLAT = np.mean(extracted_lat_ave[p,:])\n",
    "            FLON = np.mean(extracted_long_ave[p,:])\n",
    "            SOL_ANG = np.mean(extracted_solar_ave[p,:])\n",
    "            EMISS_ANG = np.mean(extracted_emiss_ave[p,:])\n",
    "            AZI_ANG = np.mean(extracted_azi_ave[p,:])\n",
    "            WGEOM = '1'\n",
    "\n",
    "            file.write('     '+str(NCONV)+' \\n')\n",
    "            file.write('     '+str(NAV)+' \\n')\n",
    "            file.write('     '+str(np.round(FLAT,3))+'  '+str(np.round(FLON,3))+'  '+str(np.round(SOL_ANG,3))+'  '+str(np.round(EMISS_ANG,3))+'  '+str(np.round(AZI_ANG,3))+'  '+str(WGEOM)+' \\n')\n",
    "\n",
    "            # print spectrum\n",
    "            for j in range(0,len(wavelengths)):\n",
    "                # raw spectrum value*OPAL correction*IF factor*radiance conversion\n",
    "                L_Sol_value = ufloat(F_solar[j],F_solar[j]*0.01) # assuming 1% error on solar spectrum\n",
    "                print(L_Sol_value)\n",
    "\n",
    "                # with error estimate: \n",
    "                print(wavelengths[j],spec[p][j])\n",
    "                \n",
    "                # convert I/F to radiance:\n",
    "                AU_to_cm = 1.496e13 # cm/AU\n",
    "                \n",
    "                print('Spectrum value:',ufloat(spec[p][j],error_percent*spec[p][j]))                    \n",
    "\n",
    "                if error_percent != 0:\n",
    "                    #print('Conversion factor:',L_Sol_value/(4*(math.pi**2)*(r*AU_to_cm)**2))\n",
    "                    #print('Jupiter raidance:', ufloat(spec[p][j],error_percent*spec[p][j])*(L_Sol_value/(4*(math.pi**2)*(r*AU_to_cm)**2)))\n",
    "                    spectrum_value_with_error = ufloat(spec[p][j],error_percent*spec[p][j])*L_Sol_value/(4*(math.pi**2)*(r*AU_to_cm)**2)\n",
    "                else:\n",
    "                # otherwise, use error_estimate. Found that that was often too small when using background variance to estimate.\n",
    "                    spectrum_value_with_error = ufloat(spec[p][j],error_estimate[0,j])*L_Sol_value/(4*(math.pi**2)*(r*AU_to_cm)**2)                    \n",
    "\n",
    "                # print wavelength in microns, radiance, error:\n",
    "                file.write('     '+str(round(wavel_spxs[j],4))+'  '+ str(round(spectrum_value_with_error.nominal_value,12))+'  '+str(round(spectrum_value_with_error.std_dev,12))+' \\n')\n",
    "\n",
    "                # plot radiance spectra:\n",
    "                #plt.plot(wavel_spxs[j],spectrum_value_with_error.nominal_value,'.')\n",
    "            #plt.show()\n",
    "        file.close()\n",
    "        \n",
    "        # Save a seperate text file that contains I/F spectra; assumes already in units of I/F\n",
    "        if save_IF_spec == 1:\n",
    "            file = open(IF_file_save_location,'w')\n",
    "            # save array to file\n",
    "            for p in range(0,n_bins):\n",
    "                # save geometric info at least\n",
    "                FLAT = np.mean(extracted_lat_ave[p,:])\n",
    "                FLON = np.mean(extracted_long_ave[p,:])\n",
    "                SOL_ANG = np.mean(extracted_solar_ave[p,:])\n",
    "                EMISS_ANG = np.mean(extracted_emiss_ave[p,:])\n",
    "                AZI_ANG = np.mean(extracted_azi_ave[p,:])\n",
    "                WGEOM = '1'\n",
    "                file.write('     '+str(np.round(FLAT,3))+'  '+str(np.round(FLON,3))+'  '+str(np.round(SOL_ANG,3))+'  '+str(np.round(EMISS_ANG,3))+'  '+str(np.round(AZI_ANG,3))+'  '+str(WGEOM)+' \\n')\n",
    "                for j in range(0,len(wavelengths)):\n",
    "                    file.write(str(round(wavel_spxs[j],4))+'  '+str(spec[p][j])+'  '+str(error_percent*spec[p][j])+' \\n')\n",
    "            \n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e063a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run spectrum_file_maker to generate spx files. Change variable names at top of file\n",
    "# when iterating over dates, loop over dates and change relevant variables for each date\n",
    "\n",
    "spectrum_file_maker(format_file, data_list, map_save_location, specfile, solarfile, IF_scale, plot_maps, map_load, wavelength_keyword, meancm_path, save_IF_spec, IF_file_save_location, error_percent=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0db8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b07692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here's what Emma ran to make spectra of the EZ in 2017 and 2019; settings should match this for a given date:\n",
    "\n",
    "# example I ran locally, for making spectra of EZ in 2019; using cal_* images instead of scaling it.\n",
    "'''\n",
    "spectrum_file_maker('/Users/emmadahl/Desktop/spextraction/spextraction_input_binned_lat_EZ.txt', \\\n",
    "                    '/Users/emmadahl/Desktop/spextraction/irtf_data_march_2019/image_list_2019jun1_3_cal', \\\n",
    "                    '/Users/emmadahl/Desktop/spextraction/test_maps_4/', \\\n",
    "                    '/Users/emmadahl/Desktop/irtf_spx_for_gattaca/jupiter.spx.2019jun1_30percenterror_EZ_cal_v6',\\\n",
    "                    '/Users/emmadahl/Desktop/IRTF_SpeX_Filter_Transmission_Functions/kurucz_spex_wl.dat.commented', \\\n",
    "                    wavelength_keyword='OSF', plot_maps=0, map_load=1, IF_scale=1, meancm_path='/Users/emmadahl/Desktop/spextraction/meancm/',save_IF_spec=1, IF_file_save_location='/Users/emmadahl/Desktop/spex_2017_EZ_main_cloud_constraint_outputs/2019jun1_IF',error_percent=0.3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8f8ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remaking the 2017 spectra to see what they look like w/ changes\n",
    "'''\n",
    "spectrum_file_maker('/Users/emmadahl/Desktop/spextraction/spextraction_input_binned_lat_EZ.txt', \\\n",
    "                    '/Users/emmadahl/Desktop/2017_irtf_spex_data/2017mar27_list_2', \\\n",
    "                    '/Users/emmadahl/Desktop/2017_irtf_spex_data/march_maps_2/', \\\n",
    "                    '/Users/emmadahl/Desktop/irtf_spx_for_gattaca/jupiter.spx.2017mar27_30percenterror_EZ_cal_v6',\\\n",
    "                    '/Users/emmadahl/Desktop/IRTF_SpeX_Filter_Transmission_Functions/kurucz_spex_wl.dat.commented', \\\n",
    "wavelength_keyword='OSF', plot_maps=0, map_load=1, IF_scale=0, meancm_path='/Users/emmadahl/Desktop/spextraction/meancm/', save_IF_spec=1, IF_file_save_location='/Users/emmadahl/Desktop/spex_2017_EZ_main_cloud_constraint_outputs/2017mar27_IF',error_percent=0.3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c09e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569d266a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spextraction_input_binned_lat_EZ.txt:\n",
    "\n",
    "'''\n",
    "2 # spectrum style code; 1 = within a given latitude range, over equally-sized mu bins\n",
    "-3.0 2.0 # minimum latitude, maximum latitude (planetocentric) # mess with this\n",
    "0.5 # mu_min, limit of mu. spectrum will be in between mu_min values on either side of the meridian. mu = cos(emission angle), and past 60 deg emission angle (0.5 mu) we start to worry about limb darkening. So without limb darkening corrections, stay above mu = 0.5. can change range = 0.5-1\n",
    "15 # total number of mu bins. if odd, will have center bin straddling the meridian (probably best).  If even, will have a bin split by the meridian. Can change this too\n",
    "1 # on/off switch for averaging across the meridian. 1 = will average radiance from same mu bins across meridian. 0 = will keep them seperate. Leave this as 1 for now as of 8/29\n",
    "0.0 # fraction of bin size, amount of overlap between bins (make 0 if want discrete bins w/ no overlap). Change this\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylanetary_2_env",
   "language": "python",
   "name": "pylanetary_2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
